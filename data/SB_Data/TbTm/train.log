Arguments are...
data_path: /home/Sayandeep/git_repo/chiral_gnn/data/SB_Data/TbTm/crit_all_TbTm_fold0_scaled.csv
split_path: /home/Sayandeep/git_repo/chiral_gnn/data/SB_Data/TbTm/TbTm_split0.npy
log_dir: /home/Sayandeep/git_repo/chiral_gnn/data/SB_Data/TbTm
task: regression
seed: 0
n_epochs: 5
batch_size: 50
warmup_epochs: 2.0
lr: 0.0001
num_workers: 20
no_shuffle: False
shuffle_pairs: False
gnn_type: gat
hidden_size: 300
depth: 3
dropout: 0.2
graph_pool: sum
message: sum
chiral_features: False
global_chiral_features: False
ffn_depth: 3
ffn_hidden_size: 600
rdkit_path: /home/Sayandeep/git_repo/chiral_gnn/data/SB_Data/TbTm/crit_all_TbTm_fold0_rdkit_feat.csv
n_rdkit: 14
n_out: 2
tetra: False
device: cpu

Optimizer parameters are:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.0001
    lr: 1.0494505494505495e-05
    weight_decay: 0
)

Scheduler state dict is:
num_lrs: 1
warmup_epochs: [2.]
total_epochs: [5]
steps_per_epoch: 91
init_lr: [1.e-05]
max_lr: [0.0001]
final_lr: [1.e-05]
current_step: 1
lr: [1.0494505494505495e-05]
warmup_steps: [182]
total_steps: [455]
linear_increment: [4.94505495e-07]
exponential_gamma: [0.99160109]
base_lrs: [0.0001]
last_epoch: -1
_step_count: 0
verbose: False

Starting training...
Epoch 0: Training Loss nan
Epoch 0: Validation Loss nan
Epoch 1: Training Loss nan
Epoch 1: Validation Loss nan
Arguments are...
data_path: /home/Sayandeep/git_repo/chiral_gnn/data/SB_Data/TbTm/crit_all_TbTm_fold0_scaled.csv
split_path: /home/Sayandeep/git_repo/chiral_gnn/data/SB_Data/TbTm/TbTm_split0.npy
log_dir: /home/Sayandeep/git_repo/chiral_gnn/data/SB_Data/TbTm
task: regression
seed: 0
n_epochs: 5
batch_size: 50
warmup_epochs: 2.0
lr: 0.0001
num_workers: 20
no_shuffle: False
shuffle_pairs: False
gnn_type: gat
hidden_size: 300
depth: 3
dropout: 0.2
graph_pool: sum
message: sum
chiral_features: False
global_chiral_features: False
ffn_depth: 3
ffn_hidden_size: 600
rdkit_path: /home/Sayandeep/git_repo/chiral_gnn/data/SB_Data/TbTm/crit_all_TbTm_fold0_rdkit_feat.csv
n_rdkit: 14
n_out: 2
tetra: False
device: cpu

Optimizer parameters are:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.0001
    lr: 1.0494505494505495e-05
    weight_decay: 0
)

Scheduler state dict is:
num_lrs: 1
warmup_epochs: [2.]
total_epochs: [5]
steps_per_epoch: 91
init_lr: [1.e-05]
max_lr: [0.0001]
final_lr: [1.e-05]
current_step: 1
lr: [1.0494505494505495e-05]
warmup_steps: [182]
total_steps: [455]
linear_increment: [4.94505495e-07]
exponential_gamma: [0.99160109]
base_lrs: [0.0001]
last_epoch: -1
_step_count: 0
verbose: False

Starting training...
Arguments are...
data_path: /home/Sayandeep/git_repo/chiral_gnn/data/SB_Data/TbTm/crit_all_TbTm_fold0_scaled.csv
split_path: /home/Sayandeep/git_repo/chiral_gnn/data/SB_Data/TbTm/TbTm_split0.npy
log_dir: /home/Sayandeep/git_repo/chiral_gnn/data/SB_Data/TbTm
task: regression
seed: 0
n_epochs: 5
batch_size: 50
warmup_epochs: 2.0
lr: 0.0001
num_workers: 20
no_shuffle: False
shuffle_pairs: False
gnn_type: gat
hidden_size: 300
depth: 3
dropout: 0.2
graph_pool: sum
message: sum
chiral_features: False
global_chiral_features: False
ffn_depth: 3
ffn_hidden_size: 600
rdkit_path: /home/Sayandeep/git_repo/chiral_gnn/data/SB_Data/TbTm/crit_all_TbTm_fold0_rdkit_feat.csv
n_rdkit: 14
n_out: 2
tetra: False
device: cpu

Optimizer parameters are:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.0001
    lr: 1.0494505494505495e-05
    weight_decay: 0
)

Scheduler state dict is:
num_lrs: 1
warmup_epochs: [2.]
total_epochs: [5]
steps_per_epoch: 91
init_lr: [1.e-05]
max_lr: [0.0001]
final_lr: [1.e-05]
current_step: 1
lr: [1.0494505494505495e-05]
warmup_steps: [182]
total_steps: [455]
linear_increment: [4.94505495e-07]
exponential_gamma: [0.99160109]
base_lrs: [0.0001]
last_epoch: -1
_step_count: 0
verbose: False

Starting training...
Arguments are...
data_path: /home/Sayandeep/git_repo/chiral_gnn/data/SB_Data/TbTm/crit_all_TbTm_fold0_scaled.csv
split_path: /home/Sayandeep/git_repo/chiral_gnn/data/SB_Data/TbTm/TbTm_split0.npy
log_dir: /home/Sayandeep/git_repo/chiral_gnn/data/SB_Data/TbTm
task: regression
seed: 0
n_epochs: 5
batch_size: 50
warmup_epochs: 2.0
lr: 0.0001
num_workers: 20
no_shuffle: False
shuffle_pairs: False
gnn_type: gat
hidden_size: 300
depth: 3
dropout: 0.2
graph_pool: sum
message: sum
chiral_features: False
global_chiral_features: False
ffn_depth: 3
ffn_hidden_size: 600
rdkit_path: /home/Sayandeep/git_repo/chiral_gnn/data/SB_Data/TbTm/crit_all_TbTm_fold0_rdkit_feat.csv
n_rdkit: 14
n_out: 2
tetra: False
device: cpu

Optimizer parameters are:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.0001
    lr: 1.0494505494505495e-05
    weight_decay: 0
)

Scheduler state dict is:
num_lrs: 1
warmup_epochs: [2.]
total_epochs: [5]
steps_per_epoch: 91
init_lr: [1.e-05]
max_lr: [0.0001]
final_lr: [1.e-05]
current_step: 1
lr: [1.0494505494505495e-05]
warmup_steps: [182]
total_steps: [455]
linear_increment: [4.94505495e-07]
exponential_gamma: [0.99160109]
base_lrs: [0.0001]
last_epoch: -1
_step_count: 0
verbose: False

Starting training...
Arguments are...
data_path: /home/Sayandeep/git_repo/chiral_gnn/data/SB_Data/TbTm/crit_all_TbTm_fold0_scaled.csv
split_path: /home/Sayandeep/git_repo/chiral_gnn/data/SB_Data/TbTm/TbTm_split0.npy
log_dir: /home/Sayandeep/git_repo/chiral_gnn/data/SB_Data/TbTm
task: regression
seed: 0
n_epochs: 5
batch_size: 50
warmup_epochs: 2.0
lr: 0.0001
num_workers: 20
no_shuffle: False
shuffle_pairs: False
gnn_type: gat
hidden_size: 300
depth: 3
dropout: 0.2
graph_pool: sum
message: sum
chiral_features: False
global_chiral_features: False
ffn_depth: 3
ffn_hidden_size: 600
rdkit_path: /home/Sayandeep/git_repo/chiral_gnn/data/SB_Data/TbTm/crit_all_TbTm_fold0_rdkit_feat.csv
n_rdkit: 14
n_out: 2
tetra: False
device: cpu

Optimizer parameters are:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.0001
    lr: 1.0494505494505495e-05
    weight_decay: 0
)

Scheduler state dict is:
num_lrs: 1
warmup_epochs: [2.]
total_epochs: [5]
steps_per_epoch: 91
init_lr: [1.e-05]
max_lr: [0.0001]
final_lr: [1.e-05]
current_step: 1
lr: [1.0494505494505495e-05]
warmup_steps: [182]
total_steps: [455]
linear_increment: [4.94505495e-07]
exponential_gamma: [0.99160109]
base_lrs: [0.0001]
last_epoch: -1
_step_count: 0
verbose: False

Starting training...
Epoch 0: Training Loss nan
Epoch 0: Validation Loss nan
Epoch 1: Training Loss nan
Arguments are...
data_path: /home/Sayandeep/git_repo/chiral_gnn/data/SB_Data/TbTm/crit_all_TbTm_fold0_scaled.csv
split_path: /home/Sayandeep/git_repo/chiral_gnn/data/SB_Data/TbTm/TbTm_split0.npy
log_dir: /home/Sayandeep/git_repo/chiral_gnn/data/SB_Data/TbTm
task: regression
seed: 0
n_epochs: 5
batch_size: 50
warmup_epochs: 2.0
lr: 0.0001
num_workers: 20
no_shuffle: False
shuffle_pairs: False
gnn_type: gat
hidden_size: 300
depth: 3
dropout: 0.2
graph_pool: sum
message: sum
chiral_features: False
global_chiral_features: False
ffn_depth: 3
ffn_hidden_size: 600
rdkit_path: /home/Sayandeep/git_repo/chiral_gnn/data/SB_Data/TbTm/crit_all_TbTm_fold0_rdkit_feat.csv
n_rdkit: 14
n_out: 2
tetra: False
device: cpu

Optimizer parameters are:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.0001
    lr: 1.0494505494505495e-05
    weight_decay: 0
)

Scheduler state dict is:
num_lrs: 1
warmup_epochs: [2.]
total_epochs: [5]
steps_per_epoch: 91
init_lr: [1.e-05]
max_lr: [0.0001]
final_lr: [1.e-05]
current_step: 1
lr: [1.0494505494505495e-05]
warmup_steps: [182]
total_steps: [455]
linear_increment: [4.94505495e-07]
exponential_gamma: [0.99160109]
base_lrs: [0.0001]
last_epoch: -1
_step_count: 0
verbose: False

Starting training...
Arguments are...
data_path: /home/Sayandeep/git_repo/chiral_gnn/data/SB_Data/TbTm/crit_all_TbTm_fold0_scaled.csv
split_path: /home/Sayandeep/git_repo/chiral_gnn/data/SB_Data/TbTm/TbTm_split0.npy
log_dir: /home/Sayandeep/git_repo/chiral_gnn/data/SB_Data/TbTm
task: regression
seed: 0
n_epochs: 5
batch_size: 50
warmup_epochs: 2.0
lr: 0.0001
num_workers: 20
no_shuffle: False
shuffle_pairs: False
gnn_type: gat
hidden_size: 300
depth: 3
dropout: 0.2
graph_pool: sum
message: sum
chiral_features: False
global_chiral_features: False
ffn_depth: 3
ffn_hidden_size: 600
rdkit_path: /home/Sayandeep/git_repo/chiral_gnn/data/SB_Data/TbTm/crit_all_TbTm_fold0_rdkit_feat.csv
n_rdkit: 14
n_out: 2
tetra: False
device: cpu

Optimizer parameters are:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.0001
    lr: 1.0494505494505495e-05
    weight_decay: 0
)

Scheduler state dict is:
num_lrs: 1
warmup_epochs: [2.]
total_epochs: [5]
steps_per_epoch: 91
init_lr: [1.e-05]
max_lr: [0.0001]
final_lr: [1.e-05]
current_step: 1
lr: [1.0494505494505495e-05]
warmup_steps: [182]
total_steps: [455]
linear_increment: [4.94505495e-07]
exponential_gamma: [0.99160109]
base_lrs: [0.0001]
last_epoch: -1
_step_count: 0
verbose: False

Starting training...
Epoch 0: Training Loss nan
Epoch 0: Validation Loss nan
Arguments are...
data_path: /home/Sayandeep/git_repo/chiral_gnn/data/SB_Data/TbTm/crit_all_TbTm_fold0_scaled.csv
split_path: /home/Sayandeep/git_repo/chiral_gnn/data/SB_Data/TbTm/TbTm_split0.npy
log_dir: /home/Sayandeep/git_repo/chiral_gnn/data/SB_Data/TbTm
task: regression
seed: 0
n_epochs: 5
batch_size: 50
warmup_epochs: 2.0
lr: 0.0001
num_workers: 20
no_shuffle: False
shuffle_pairs: False
gnn_type: gat
hidden_size: 300
depth: 3
dropout: 0.2
graph_pool: sum
message: sum
chiral_features: False
global_chiral_features: False
ffn_depth: 3
ffn_hidden_size: 600
rdkit_path: /home/Sayandeep/git_repo/chiral_gnn/data/SB_Data/TbTm/crit_all_TbTm_fold0_rdkit_feat.csv
n_rdkit: 14
n_out: 2
tetra: False
device: cpu

Optimizer parameters are:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.0001
    lr: 1.0494505494505495e-05
    weight_decay: 0
)

Scheduler state dict is:
num_lrs: 1
warmup_epochs: [2.]
total_epochs: [5]
steps_per_epoch: 91
init_lr: [1.e-05]
max_lr: [0.0001]
final_lr: [1.e-05]
current_step: 1
lr: [1.0494505494505495e-05]
warmup_steps: [182]
total_steps: [455]
linear_increment: [4.94505495e-07]
exponential_gamma: [0.99160109]
base_lrs: [0.0001]
last_epoch: -1
_step_count: 0
verbose: False

Starting training...
Epoch 0: Training Loss nan
Epoch 0: Validation Loss nan
Arguments are...
data_path: /home/Sayandeep/git_repo/chiral_gnn/data/SB_Data/TbTm/crit_all_TbTm_fold0_scaled.csv
split_path: /home/Sayandeep/git_repo/chiral_gnn/data/SB_Data/TbTm/TbTm_split0.npy
log_dir: /home/Sayandeep/git_repo/chiral_gnn/data/SB_Data/TbTm
task: regression
seed: 0
n_epochs: 5
batch_size: 50
warmup_epochs: 2.0
lr: 0.0001
num_workers: 20
no_shuffle: False
shuffle_pairs: False
gnn_type: gat
hidden_size: 300
depth: 3
dropout: 0.2
graph_pool: sum
message: sum
chiral_features: False
global_chiral_features: False
ffn_depth: 3
ffn_hidden_size: 600
rdkit_path: /home/Sayandeep/git_repo/chiral_gnn/data/SB_Data/TbTm/crit_all_TbTm_fold0_rdkit_feat.csv
n_rdkit: 14
n_out: 2
tetra: False
device: cpu

Optimizer parameters are:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.0001
    lr: 1.0494505494505495e-05
    weight_decay: 0
)

Scheduler state dict is:
num_lrs: 1
warmup_epochs: [2.]
total_epochs: [5]
steps_per_epoch: 91
init_lr: [1.e-05]
max_lr: [0.0001]
final_lr: [1.e-05]
current_step: 1
lr: [1.0494505494505495e-05]
warmup_steps: [182]
total_steps: [455]
linear_increment: [4.94505495e-07]
exponential_gamma: [0.99160109]
base_lrs: [0.0001]
last_epoch: -1
_step_count: 0
verbose: False

Starting training...
Arguments are...
data_path: /home/Sayandeep/git_repo/chiral_gnn/data/SB_Data/TbTm/crit_all_TbTm_fold0_scaled.csv
split_path: /home/Sayandeep/git_repo/chiral_gnn/data/SB_Data/TbTm/TbTm_split0.npy
log_dir: /home/Sayandeep/git_repo/chiral_gnn/data/SB_Data/TbTm
task: regression
seed: 0
n_epochs: 5
batch_size: 50
warmup_epochs: 2.0
lr: 0.0001
num_workers: 20
no_shuffle: False
shuffle_pairs: False
gnn_type: gat
hidden_size: 300
depth: 3
dropout: 0.2
graph_pool: sum
message: sum
chiral_features: False
global_chiral_features: False
ffn_depth: 3
ffn_hidden_size: 600
rdkit_path: /home/Sayandeep/git_repo/chiral_gnn/data/SB_Data/TbTm/crit_all_TbTm_fold0_rdkit_feat.csv
n_rdkit: 14
n_out: 2
tetra: False
device: cpu

Optimizer parameters are:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.0001
    lr: 1.0494505494505495e-05
    weight_decay: 0
)

Scheduler state dict is:
num_lrs: 1
warmup_epochs: [2.]
total_epochs: [5]
steps_per_epoch: 91
init_lr: [1.e-05]
max_lr: [0.0001]
final_lr: [1.e-05]
current_step: 1
lr: [1.0494505494505495e-05]
warmup_steps: [182]
total_steps: [455]
linear_increment: [4.94505495e-07]
exponential_gamma: [0.99160109]
base_lrs: [0.0001]
last_epoch: -1
_step_count: 0
verbose: False

Starting training...
Arguments are...
data_path: /home/Sayandeep/git_repo/chiral_gnn/data/SB_Data/TbTm/crit_all_TbTm_fold0_scaled.csv
split_path: /home/Sayandeep/git_repo/chiral_gnn/data/SB_Data/TbTm/TbTm_split0.npy
log_dir: /home/Sayandeep/git_repo/chiral_gnn/data/SB_Data/TbTm
task: regression
seed: 0
n_epochs: 5
batch_size: 50
warmup_epochs: 2.0
lr: 0.0001
num_workers: 20
no_shuffle: False
shuffle_pairs: False
gnn_type: gat
hidden_size: 300
depth: 3
dropout: 0.2
graph_pool: sum
message: sum
chiral_features: False
global_chiral_features: False
ffn_depth: 3
ffn_hidden_size: 600
rdkit_path: /home/Sayandeep/git_repo/chiral_gnn/data/SB_Data/TbTm/crit_all_TbTm_fold0_rdkit_feat.csv
n_rdkit: 14
n_out: 2
tetra: False
device: cpu

Optimizer parameters are:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.0001
    lr: 1.0494505494505495e-05
    weight_decay: 0
)

Scheduler state dict is:
num_lrs: 1
warmup_epochs: [2.]
total_epochs: [5]
steps_per_epoch: 91
init_lr: [1.e-05]
max_lr: [0.0001]
final_lr: [1.e-05]
current_step: 1
lr: [1.0494505494505495e-05]
warmup_steps: [182]
total_steps: [455]
linear_increment: [4.94505495e-07]
exponential_gamma: [0.99160109]
base_lrs: [0.0001]
last_epoch: -1
_step_count: 0
verbose: False

Starting training...
Arguments are...
data_path: /home/Sayandeep/git_repo/chiral_gnn/data/SB_Data/TbTm/crit_all_TbTm_fold0_scaled.csv
split_path: /home/Sayandeep/git_repo/chiral_gnn/data/SB_Data/TbTm/TbTm_split0.npy
log_dir: /home/Sayandeep/git_repo/chiral_gnn/data/SB_Data/TbTm
task: regression
seed: 0
n_epochs: 5
batch_size: 50
warmup_epochs: 2.0
lr: 0.0001
num_workers: 20
no_shuffle: False
shuffle_pairs: False
gnn_type: gat
hidden_size: 300
depth: 3
dropout: 0.2
graph_pool: sum
message: sum
chiral_features: False
global_chiral_features: False
ffn_depth: 3
ffn_hidden_size: 600
rdkit_path: /home/Sayandeep/git_repo/chiral_gnn/data/SB_Data/TbTm/crit_all_TbTm_fold0_rdkit_feat.csv
n_rdkit: 14
n_out: 2
tetra: False
device: cpu

Optimizer parameters are:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.0001
    lr: 1.0494505494505495e-05
    weight_decay: 0
)

Scheduler state dict is:
num_lrs: 1
warmup_epochs: [2.]
total_epochs: [5]
steps_per_epoch: 91
init_lr: [1.e-05]
max_lr: [0.0001]
final_lr: [1.e-05]
current_step: 1
lr: [1.0494505494505495e-05]
warmup_steps: [182]
total_steps: [455]
linear_increment: [4.94505495e-07]
exponential_gamma: [0.99160109]
base_lrs: [0.0001]
last_epoch: -1
_step_count: 0
verbose: False

Starting training...
Epoch 0: Training Loss nan
Epoch 0: Validation Loss nan
Arguments are...
data_path: /home/Sayandeep/git_repo/chiral_gnn/data/SB_Data/TbTm/crit_all_TbTm_fold0_scaled.csv
split_path: /home/Sayandeep/git_repo/chiral_gnn/data/SB_Data/TbTm/TbTm_split0.npy
log_dir: /home/Sayandeep/git_repo/chiral_gnn/data/SB_Data/TbTm
task: regression
seed: 0
n_epochs: 5
batch_size: 50
warmup_epochs: 2.0
lr: 0.0001
num_workers: 20
no_shuffle: False
shuffle_pairs: False
gnn_type: gat
hidden_size: 300
depth: 3
dropout: 0.2
graph_pool: sum
message: sum
chiral_features: False
global_chiral_features: False
ffn_depth: 3
ffn_hidden_size: 600
rdkit_path: /home/Sayandeep/git_repo/chiral_gnn/data/SB_Data/TbTm/crit_all_TbTm_fold0_rdkit_feat.csv
n_rdkit: 14
n_out: 2
tetra: False
device: cpu

Optimizer parameters are:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.0001
    lr: 1.0494505494505495e-05
    weight_decay: 0
)

Scheduler state dict is:
num_lrs: 1
warmup_epochs: [2.]
total_epochs: [5]
steps_per_epoch: 91
init_lr: [1.e-05]
max_lr: [0.0001]
final_lr: [1.e-05]
current_step: 1
lr: [1.0494505494505495e-05]
warmup_steps: [182]
total_steps: [455]
linear_increment: [4.94505495e-07]
exponential_gamma: [0.99160109]
base_lrs: [0.0001]
last_epoch: -1
_step_count: 0
verbose: False

Starting training...
Epoch 0: Training Loss 1.1288598918966148
Epoch 0: Validation Loss nan
Arguments are...
data_path: /home/Sayandeep/git_repo/chiral_gnn/data/SB_Data/TbTm/crit_all_TbTm_fold0_scaled.csv
split_path: /home/Sayandeep/git_repo/chiral_gnn/data/SB_Data/TbTm/TbTm_split0.npy
log_dir: /home/Sayandeep/git_repo/chiral_gnn/data/SB_Data/TbTm
task: regression
seed: 0
n_epochs: 5
batch_size: 50
warmup_epochs: 2.0
lr: 0.0001
num_workers: 20
no_shuffle: False
shuffle_pairs: False
gnn_type: gat
hidden_size: 300
depth: 3
dropout: 0.2
graph_pool: sum
message: sum
chiral_features: False
global_chiral_features: False
ffn_depth: 3
ffn_hidden_size: 600
rdkit_path: /home/Sayandeep/git_repo/chiral_gnn/data/SB_Data/TbTm/crit_all_TbTm_fold0_rdkit_feat.csv
n_rdkit: 14
n_out: 2
tetra: False
device: cpu

Optimizer parameters are:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.0001
    lr: 1.0494505494505495e-05
    weight_decay: 0
)

Scheduler state dict is:
num_lrs: 1
warmup_epochs: [2.]
total_epochs: [5]
steps_per_epoch: 91
init_lr: [1.e-05]
max_lr: [0.0001]
final_lr: [1.e-05]
current_step: 1
lr: [1.0494505494505495e-05]
warmup_steps: [182]
total_steps: [455]
linear_increment: [4.94505495e-07]
exponential_gamma: [0.99160109]
base_lrs: [0.0001]
last_epoch: -1
_step_count: 0
verbose: False

Starting training...
Epoch 0: Training Loss 1.1288598918966148
Epoch 0: Validation Loss 1.007470741045938
Epoch 1: Training Loss 0.963269605976154
Epoch 1: Validation Loss 0.9533846495829398
Epoch 2: Training Loss 0.8272673646355784
Epoch 2: Validation Loss 0.74642685874052
Epoch 3: Training Loss 0.7443755572466683
Epoch 3: Validation Loss 0.6882916709958427
Epoch 4: Training Loss 0.6997518372015841
Epoch 4: Validation Loss 0.6769209452412432
Best Validation Loss 0.6769209452412432 on Epoch 4
Test Loss 0.6983318810594056
